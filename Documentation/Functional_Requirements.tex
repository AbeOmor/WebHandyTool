\documentclass[12pt]{article}
\usepackage{xcolor} % for different colour comments
\usepackage[left=15mm,right=15mm,top=1in,bottom=1in]{geometry}
\usepackage{framed}
\usepackage{graphicx}
\graphicspath{ {images/} }
%% Comments
\newif\ifcomments\commentsfalse %i replaced comments true by comment false so the comments will be hidden

\ifcomments
\newcommand{\authornote}[3]{\textcolor{#1}{[#3 ---#2]}}
\newcommand{\todo}[1]{\textcolor{red}{[TODO: #1]}}
\else
\newcommand{\authornote}[3]{}
\newcommand{\todo}[1]{}
\fi

\newcommand{\wss}[1]{\authornote{magenta}{SS}{#1}}
\newcommand{\hm}[1]{\authornote{blue}{HM}{#1}} %Hediyeh
\newcommand{\tz}[1]{\authornote{blue}{TZ}{#1}} %Tahereh
\newcommand{\pl}[1]{\authornote{blue}{PL}{#1}} %Peng

\begin{document}
\title{pylinkvalidator \\
 Problem Statement : newAGEtech, Group H }
\author{Genevieve Okon (Okong), Abraham Omorogbe(Omorogoa),\\
 Eric Le Forti(Leforte)}
\date{\today}
\maketitle

	
\section{The Scope of the Work }

\subsection{The Current Situation}

It is difficult often difficult for users of the internet to navigates through the web, it can be difficult to locate pertinent information using standard methods of search such as search engines. A web crawler and link validator will allow users quickly traverse the web and quickly and effectively locate information from different web pages and information about these web pages. A web crawler and link validator will increase users efficiency, it will be able to save money and resources due to users spending less time and effort surfing the web for information.\newline

\subsection{The Context of the Work }
\includegraphics[scale=0.6]{Work_Context_Diagram}
\subsection{Work Partitioning}
The table \ref{table:Business Event List} is a Business Event List.

\begin{table}[h!]
    \begin{tabular}{| p{5cm} | p{5cm} | p{5cm} |}    \hline
    Event Name &	Input and Output	 & Summary\\ \hline
    User requests information about a website	& Website start link to crawl and specific info gathering setting (In)	& Have a starting point for crawler can gather resources from. \\ \hline
    User requests information from a website	 & Website start link to crawl and specific status checking setting (In)	& Have a starting point for crawler, it can continue to verify any links associate with initial website.\\ \hline
    Traverse website	& Links from initial retrieved website (In)	& Has the abilities to reach website that are associated with starting link \\ \hline
    
    List of crawled websites status code & 	A list of all the site that were visited (Out)	& Has the ability to show the user the status codes from crawled websites, can verify if links are down. \\ \hline
     
      List of information from website	& All resources from a website (Out)	& Has the ability to show the user all the information from website they have elected. \\ \hline
      
    \end{tabular}
    \caption{Business Event List}
\label{table:Business Event List}
\end{table}


\section{The Scope of the Product }
\subsection{Product Boundary }
\includegraphics[scale=0.75]{pylinkvalidator_Use_Case_Diagram}
\subsection{Product Use Case List  }
\begin{itemize}
  \item Locate Information and related websites
  \item Automatically download resources from a web page
  \item View list of status codes from website 
  \item Specify website to crawl
  \item Audit Content on companies website 
  \item Map website structure
  \item Return website status code
\end{itemize}

\subsection{Individual Product Use Cases  }
\begin{enumerate}
  \item  Product Use Case Name: Locate Information and related websites \\
  Trigger: User requests a website to be processed \\Preconditions: User has to specify website that exists\\Interested Stakeholders: \\Actors: User, Website\\Outcome: If the website is a valid HTML page, the link on the page a verified and information is gathered (text-based), if the website is an invalid, an error is displayed.\\
  \item Product Use Case Name: Automatically download resources from a web page.\\
  Trigger: User types in command to download dataPreconditions: Web server has to have existing resources available, and user needs permission to the website.\\Interested Stakeholders: \\Actors: User, Web Server, Website\\Outcome: User has all the resources from the website (images, attached files etc.).\\
  \item Product Use Case Name: View list of status codes from website
  Trigger: User types in command to show status code\\Preconditions: User must have Internet connection and must have entered a valid URL.\\Interested Stakeholders: \\Actors: User\\Outcome: User can see a list of all the status codes on websites associated with the initial site.\\
  \item Product Use Case Name: Specify website to crawl\\Trigger: User types in a website to crawl\\Preconditions: User must have Internet connection and a valid website to crawl.\\Interested Stakeholders:\\Actors User, Website\\Outcome: The web crawler begins to traverse through website.\\
  \item Product Use Case Name: Audit Content on companies website \\Trigger: User requests a list of all websites associated with the starting point.\\Preconditions: User must enter a website with valid <a href=””> </a> tags\\Interested Stakeholders: \\Actors: User, Website\\Outcome: The users can see a list of on the links and resources attached to every webpage associated with the specified domain.\\
  \item Product Use Case Name: Map website structure\\Trigger: User requests a structure of a website\\Preconditions: Must have entered a valid URL/HTML.\\Interested Stakeholders: \\Actors: Web Server, Website\\Outcome: User is shown the website file structure.\\
  \item Product Use Case Name: Return website status code\\Trigger: Web crawler sends request query to web server\\Preconditions: Web server must respond to queries with status codes. \\Interested Stakeholders: \\Actors: Web Server\\Outcome: The web server responds with correct status code.\\
\end{enumerate}

\section{Functional and Data Requirements }
\subsection{Functional Requirements }

\begin{framed}

\textbf{Requirement \#}: 1 \hfill \textbf{Requirement Type}: 9 \hfill\textbf{Event/Use case \#}: 7 \hfill\\\textbf{Description}:  The product shall return the status code of websites\\\textbf{Rationale}: To allow the users verify the status of all the pages on a website. Checks if it is offline, Unauthorized etc.\\\textbf{Originator}: Abraham Omorogbe\\\textbf{Fit Criterion}: The web crawler displays an error with it reaches an website that is not online (Status Code: 200 ) and the status code related the error (Status Code: 400,500 etc.).	\\\textbf{Customer Satisfaction}: 5 \hfill 	\textbf{Customer Dissatisfaction}: 5 \hfill\\\textbf{Priority}: High \hfill \textbf{Conflicts}: None \hfill 		\\\textbf{Supporting Material}: None\\\textbf{History}: Created October 9, 2015 \hfill	 \textbf{Volere}\hfill
\end{framed}

\begin{framed}
\textbf{Requirement \#}: 2 \hfill \textbf{Requirement Type}: 9 \hfill\textbf{Event/Use case \#}: 2 \hfill\\\textbf{Description}:  The product shall download resources from a website\\\textbf{Rationale}: To allow users quickly download all the HTML, images, CSS and other data related to a website\\\textbf{Originator}: Abraham Omorogbe\\\textbf{Fit Criterion}:  The download resources can be retrieved and used.\\\textbf{Customer Satisfaction}: 5 \hfill 	\textbf{Customer Dissatisfaction}: 5 \hfill\\\textbf{Priority}: High \hfill \textbf{Conflicts}: None \hfill 		\\\textbf{Supporting Material}: None\\\textbf{History}: Created October 9, 2015 \hfill	 \textbf{Volere}\hfill
\end{framed}

\begin{framed}
\textbf{Requirement \#}: 3 \hfill \textbf{Requirement Type}: 9 \hfill\textbf{Event/Use case \#}: 1,5 \hfill\\\textbf{Description}:  The product shall have adjustable search max-depth\\\textbf{Rationale}: To allow the user specific how many layers of the website, the user want to analyse. Helps with auditing and finding related sites.\\\textbf{Originator}: Abraham Omorogbe\\\textbf{Fit Criterion}: The web crawler only show top-level links when max-depth is at 0, and shows more levels of the website when depth > 0\\\textbf{Customer Satisfaction}: 5 \hfill 	\textbf{Customer Dissatisfaction}: 5 \hfill\\\textbf{Priority}: High \hfill \textbf{Conflicts}: None \hfill 		\\\textbf{Supporting Material}: None\\\textbf{History}: Created October 9, 2015 \hfill	 \textbf{Volere}\hfill
\end{framed}

\begin{framed}
\textbf{Requirement \#}: 4 \hfill \textbf{Requirement Type}: 9 \hfill\textbf{Event/Use case \#}: 4 \hfill\\\textbf{Description}:  The product shall always users enter a starting URL or local HTML page\\\textbf{Rationale}: To allow the user crawl any website that is on or offline\\\textbf{Originator}: Abraham Omorogbe\\\textbf{Fit Criterion}: The crawler starts from the inputted website.\\\textbf{Customer Satisfaction}: 5 \hfill 	\textbf{Customer Dissatisfaction}: 5 \hfill\\\textbf{Priority}: High \hfill \textbf{Conflicts}: None \hfill 		\\\textbf{Supporting Material}: None\\\textbf{History}: Created October 9, 2015 \hfill	 \textbf{Volere}\hfill
\end{framed}

\begin{framed}
\textbf{Requirement \#}: 5 \hfill \textbf{Requirement Type}: 9 \hfill\textbf{Event/Use case \#}: 3,5 \hfill\\\textbf{Description}:  The product shall display all a report of all found results and corresponding websites\\\textbf{Rationale}: This is one of the main functionality of a web crawl, the application should be able to start at one sites and find related links. List will be used to gather info and check status codes	\\\textbf{Originator}: Abraham Omorogbe\\\textbf{Fit Criterion}: The reports are accurate. All link that are reported as status code 404 are offline and all details of resource match source website.\\\textbf{Customer Satisfaction}: 5 \hfill 	\textbf{Customer Dissatisfaction}: 5 \hfill\\\textbf{Priority}: High \hfill \textbf{Conflicts}: None \hfill 		\\\textbf{Supporting Material}: None\\\textbf{History}: Created October 9, 2015 \hfill	 \textbf{Volere}\hfill
\end{framed}

\begin{framed}
\textbf{Requirement \#}: 6 \hfill \textbf{Requirement Type}: 9 \hfill\textbf{Event/Use case \#}: 5,6 \hfill\\\textbf{Description}:  The product shall able to display website’s structure\\\textbf{Rationale}: Give to users a visually representation of the structure for entered website.\\\textbf{Originator}: Abraham Omorogbe\\\textbf{Fit Criterion}: The web structure matches the structure of inputted website.	\\\textbf{Customer Satisfaction}: 5 \hfill 	\textbf{Customer Dissatisfaction}: 5 \hfill\\\textbf{Priority}: High \hfill \textbf{Conflicts}: None \hfill 		\\\textbf{Supporting Material}: None\\\textbf{History}: Created October 9, 2015 \hfill	 \textbf{Volere}\hfill
\end{framed}

\begin{framed}
\textbf{Requirement \#}: 7 \hfill \textbf{Requirement Type}: 9 \hfill\textbf{Event/Use case \#}: 1 \hfill\\\textbf{Description}:  The product shall find specific information from specified websites and related sites\\\textbf{Rationale}: To allow user enter details they are looking for, and the application can return data based on starting URL.\\\textbf{Originator}: Abraham Omorogbe\\\textbf{Fit Criterion}: Returned websites most be related with inputted website and information return must match what the user specified.\\\textbf{Customer Satisfaction}: 5 \hfill 	\textbf{Customer Dissatisfaction}: 5 \hfill\\\textbf{Priority}: High \hfill \textbf{Conflicts}: None \hfill 		\\\textbf{Supporting Material}: None\\\textbf{History}: Created October 9, 2015 \hfill	 \textbf{Volere}\hfill
\end{framed}

\subsection{Data Requirements }
\begin{itemize}
  \item Valid HTML pages to be parsed and crawl through.
  \item The websites resources that the crawler can download and check for. 
\end{itemize}


\noindent \wss{This is an example comment.  You can turn comments off by replacing
  commentstrue by commentsfalse.}\\
\hm{Sample comment}\\
\tz{Sample comment}\\
\pl{Sample comment}

\end{document}